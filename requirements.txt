# Requirements automatically generated by pigar.
# https://github.com/damnever/pigar

# playground/baselines_changes/run.py: 16
# playground/baselines_changes/trpo_mpi/trpo_mpi.py: 1,2,3,6,9,10,11,12,13
baselines == 0.1.5

# examples/rl_examples/tabular_q_param_search.py: 9
# examples/tabular_q_metrics_storage.py: 8
comet-ml == 1.0.31

# irl_benchmark/irl/algorithms/appr_irl.py: 4
# old/irl/algorithms/appr/appr_irl.py: 1
cvxpy == 1.0.10

# examples/aaa.py: 1
# examples/frozen_relent.py: 1
# examples/frozen_svm.py: 1
# examples/meirl_experiments.py: 1
# examples/metrics_experiments.py: 1
# examples/pendulum_svm.py: 1
# examples/rl_examples/tabular_q_param_search.py: 4
# examples/tabular_q_metrics_storage.py: 2
# irl_benchmark/experiment/run.py: 4
# irl_benchmark/irl/algorithms/appr_irl.py: 5
# irl_benchmark/irl/algorithms/base_algorithm.py: 5
# irl_benchmark/irl/algorithms/mce_irl.py: 1,5
# irl_benchmark/irl/algorithms/me_irl.py: 5,6
# irl_benchmark/irl/collect/__init__.py: 6
# irl_benchmark/irl/feature/feature_wrapper.py: 7
# irl_benchmark/irl/reward/reward_function.py: 6,7
# irl_benchmark/irl/reward/reward_wrapper.py: 4
# irl_benchmark/irl/reward/truth.py: 2
# irl_benchmark/rl/algorithms/base_algorithm.py: 6
# irl_benchmark/rl/algorithms/random_agent.py: 5
# irl_benchmark/rl/algorithms/value_iteration.py: 3
# irl_benchmark/tests/rl_value_iteration_test.py: 1
# irl_benchmark/tests/utils_wrapper_test.py: 1,2,3
# irl_benchmark/utils/wrapper.py: 6,7
# old/envs/__init__.py: 1,2
# old/irl/algorithms/maxcausalent/mce.py: 1,3
# old/irl/algorithms/maxent/me_irl.py: 1
# old/irl/algorithms/relent/relent.py: 3
# old/irl/feature/feature_wrapper.py: 3
# old/irl/reward/reward_function.py: 7,8,9
# old/irl/reward/reward_wrapper.py: 1
# old/irl/reward/truth.py: 2
# old/metrics/reward_l2_loss.py: 4
# old/rl/algorithms/ppo.py: 2
# old/utils/utils.py: 6
# playground/baselines_changes/run.py: 4
# playground/baselines_changes/trpo_mpi/trpo_mpi.py: 16,17
gym == 0.10.8

# playground/baselines_changes/run.py: 117
# gym-retro == 0.6.0

# examples/aaa.py: 4,5,6,7,8,9
# examples/frozen_relent.py: 14
# examples/frozen_svm.py: 4,5,6,7,8,9
# examples/meirl_experiments.py: 4,5,6,7,8,9,10
# examples/metrics_experiments.py: 4,5,6,7,8,9,10,11
# examples/pendulum_svm.py: 8,9,10,11,12,13
# examples/rl_examples/tabular_q_param_search.py: 7
# examples/slm.py: 11
# examples/tabular_q_metrics_storage.py: 5,6
# generate_expert_data.py: 2,3,4
# irl_benchmark/experiment/run.py: 9,11,12,13
# irl_benchmark/irl/algorithms/appr_irl.py: 11,12,13,14,15,16
# irl_benchmark/irl/algorithms/base_algorithm.py: 14,15
# irl_benchmark/irl/algorithms/mce_irl.py: 13
# irl_benchmark/irl/algorithms/me_irl.py: 15
# irl_benchmark/irl/collect/__init__.py: 11
# irl_benchmark/irl/feature/feature_wrapper.py: 10
# irl_benchmark/irl/reward/reward_function.py: 10,11
# irl_benchmark/irl/reward/reward_wrapper.py: 10
# irl_benchmark/irl/reward/truth.py: 5
# irl_benchmark/metrics/avg_traj_return.py: 1,2,3
# irl_benchmark/metrics/base_metric.py: 5
# irl_benchmark/metrics/feature_count_inf.py: 3,4,5
# irl_benchmark/metrics/feature_count_l2.py: 3,4,5
# irl_benchmark/rl/algorithms/base_algorithm.py: 9
# irl_benchmark/rl/algorithms/random_agent.py: 8,9
# irl_benchmark/rl/algorithms/value_iteration.py: 8
# irl_benchmark/tests/irl_algs_runnable_test.py: 3,4,5,6,7,8,9
# irl_benchmark/tests/rl_value_iteration_test.py: 4,5,6,7
# irl_benchmark/tests/utils_general_test.py: 4
# irl_benchmark/tests/utils_irl_test.py: 3,4,5
# irl_benchmark/tests/utils_rl_test.py: 1
# irl_benchmark/tests/utils_wrapper_test.py: 10
# irl_benchmark/utils/irl.py: 7
# irl_benchmark/utils/wrapper.py: 10,11,12
# main.py: 1,2,3,4,5,6,7,8,9
# old/config.py: 2,3
# old/envs/__init__.py: 4,5
# old/irl/algorithms/appr/appr_irl.py: 5,6,7,8
# old/irl/algorithms/base_algorithm.py: 3,4
# old/irl/algorithms/lp/LP3.py: 1
# old/irl/algorithms/maxcausalent/mce.py: 7
# old/irl/algorithms/maxent/me_irl.py: 5,6,7
# old/irl/algorithms/relent/relent.py: 6,7,8,9,10,11,12,13
# old/irl/feature/feature_wrapper.py: 6
# old/irl/reward/reward_wrapper.py: 3,4,5,6
# old/irl/reward/truth.py: 5
# old/metrics/inverse_learning_error.py: 3,4
# old/metrics/reward_l2_loss.py: 6,8
# old/rl/algorithms/base_algorithm.py: 8
# old/rl/algorithms/ppo.py: 5
# old/rl/algorithms/tabular_q.py: 6
# old/rl/algorithms/value_iteration.py: 6
# old/utils/utils.py: 8,9,10
irl_benchmark == 0.1

# old/utils/utils.py: 4
matplotlib == 3.0.1

# irl_benchmark/irl/collect/__init__.py: 7
msgpack == 0.5.6

# irl_benchmark/irl/collect/__init__.py: 8
msgpack-numpy == 0.4.4.1

# examples/aaa.py: 2
# examples/frozen_relent.py: 2
# examples/frozen_svm.py: 2
# examples/meirl_experiments.py: 2
# examples/metrics_experiments.py: 2
# examples/pendulum_svm.py: 3
# examples/rl_examples/tabular_q_param_search.py: 5
# examples/tabular_q_metrics_storage.py: 3
# irl_benchmark/irl/algorithms/appr_irl.py: 6
# irl_benchmark/irl/algorithms/base_algorithm.py: 6
# irl_benchmark/irl/algorithms/mce_irl.py: 2
# irl_benchmark/irl/algorithms/me_irl.py: 7
# irl_benchmark/irl/feature/feature_wrapper.py: 8
# irl_benchmark/irl/reward/reward_function.py: 8
# irl_benchmark/irl/reward/reward_wrapper.py: 5
# irl_benchmark/irl/reward/truth.py: 3
# irl_benchmark/metrics/feature_count_inf.py: 1
# irl_benchmark/metrics/feature_count_l2.py: 1
# irl_benchmark/rl/algorithms/base_algorithm.py: 7
# irl_benchmark/rl/algorithms/random_agent.py: 6
# irl_benchmark/rl/algorithms/value_iteration.py: 4
# irl_benchmark/tests/irl_algs_runnable_test.py: 1
# irl_benchmark/tests/rl_value_iteration_test.py: 2
# irl_benchmark/tests/utils_general_test.py: 1
# irl_benchmark/tests/utils_irl_test.py: 1
# irl_benchmark/tests/utils_wrapper_test.py: 4
# irl_benchmark/utils/general.py: 5
# irl_benchmark/utils/irl.py: 4
# irl_benchmark/utils/rl.py: 4
# irl_benchmark/utils/wrapper.py: 8
# old/irl/algorithms/appr/appr_irl.py: 2
# old/irl/algorithms/base_algorithm.py: 1
# old/irl/algorithms/maxcausalent/mce.py: 2
# old/irl/algorithms/maxent/me_irl.py: 2
# old/irl/algorithms/relent/relent.py: 2
# old/irl/feature/feature_wrapper.py: 4
# old/irl/reward/reward_function.py: 11
# old/irl/reward/truth.py: 3
# old/metrics/inverse_learning_error.py: 1
# old/metrics/reward_l2_loss.py: 1
# old/rl/algorithms/base_algorithm.py: 1
# old/rl/algorithms/ppo.py: 3
# old/rl/algorithms/tabular_q.py: 1
# old/rl/algorithms/val_iter.py: 1
# old/rl/algorithms/value_iteration.py: 1
# old/utils/utils.py: 5
# playground/baselines_changes/run.py: 7
# playground/baselines_changes/trpo_mpi/trpo_mpi.py: 4
numpy == 1.15.3

# docs/conf.py: 51
recommonmark == 0.4.0

# setup.py: 1
setuptools == 40.4.3

# playground/baselines_changes/run.py: 6
# playground/baselines_changes/trpo_mpi/trpo_mpi.py: 4
tensorflow == 1.11.0

# irl_benchmark/tests/utils_general_test.py: 2
# irl_benchmark/utils/general.py: 6
# old/metrics/reward_l2_loss.py: 3
torch == 0.4.1.post2

# irl_benchmark/irl/collect/__init__.py: 9
tqdm == 4.28.1
